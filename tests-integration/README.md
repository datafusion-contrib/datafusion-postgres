# DataFusion PostgreSQL Integration Tests

This directory contains integration tests for PostgreSQL compatibility features in datafusion-postgres.

## Test Files

### Core Tests
- **`test_csv.py`** - Enhanced CSV data loading with PostgreSQL compatibility, transaction support, and pg_catalog testing
- **`test_parquet.py`** - Enhanced Parquet data loading with advanced data types, array support, and complex queries  
- **`test_transactions.py`** - Comprehensive transaction support testing (BEGIN, COMMIT, ROLLBACK, failed transactions)

### Test Runner
- **`test.sh`** - Main test runner script that executes all enhanced tests

## Features Tested

### ğŸ” Transaction Support
- âœ… **Complete transaction lifecycle** - BEGIN, COMMIT, ROLLBACK, ABORT
- âœ… **Transaction variants** - BEGIN WORK, COMMIT TRANSACTION, START TRANSACTION, END, etc.
- âœ… **Failed transaction handling** - Proper error codes (25P01), query blocking until rollback
- âœ… **Transaction state persistence** - Multiple queries within single transaction
- âœ… **Edge cases** - COMMIT outside transaction, nested BEGIN, COMMIT in failed transaction

### ğŸ—‚ï¸ Enhanced pg_catalog System Tables  
- âœ… **pg_type** - PostgreSQL data types (16+ core types: bool, int2, int4, int8, float4, float8, text, char, bytea, date, timestamp, time, interval, uuid, json, jsonb)
- âœ… **pg_class** - Table and relation metadata with **proper table type detection** (relkind: 'r' for regular, 'v' for views)
- âœ… **pg_attribute** - Column information with proper attnum, atttypid, etc.
- âœ… **pg_proc** - Function metadata for PostgreSQL compatibility functions
- âœ… **pg_namespace** - Schema information (public, pg_catalog, information_schema)
- âœ… **pg_database** - Database metadata

### ğŸ”§ PostgreSQL Functions
- âœ… **version()** - Returns DataFusion PostgreSQL version string
- âœ… **current_schema()** - Returns current schema name
- âœ… **current_schemas(boolean)** - Returns schema search path
- âœ… **pg_get_userbyid(oid)** - Returns user name for given OID
- âœ… **has_table_privilege(user, table, privilege)** - Checks table privileges

### ğŸ—ï¸ Advanced Data Type Support
- âœ… **Array types** - BOOL_ARRAY, INT2_ARRAY, INT4_ARRAY, INT8_ARRAY, FLOAT4_ARRAY, FLOAT8_ARRAY, TEXT_ARRAY
- âœ… **Advanced types** - MONEY, INET, MACADDR for financial/network data
- âœ… **Complex data types** - Nested lists, maps, union types, dictionary types
- âœ… **Enhanced parameter types** - TIME, UUID, JSON, JSONB, INTERVAL in prepared statements
- âœ… **Proper Arrow to PostgreSQL type mapping**
- âœ… **Fixed encoder issues** - Time32/Time64, LargeUtf8, Decimal256, Duration array types

### ğŸ›¡ï¸ Error Handling & Compatibility
- âœ… **PostgreSQL-compatible error codes** - "22003" for numeric_value_out_of_range, "25P01" for aborted transactions
- âœ… **Proper error message formatting** - Standard PostgreSQL error structure
- âœ… **Graceful handling** - Invalid queries, data type conversions, transaction failures
- âœ… **Failed transaction recovery** - Proper blocking and rollback mechanisms

### ğŸ“‹ information_schema Compatibility
- âœ… **information_schema.tables** - Table metadata with proper table types
- âœ… **information_schema.columns** - Column metadata with data type information
- âœ… **PostgreSQL tool compatibility** - Works with tools expecting standard system catalogs
- âœ… **JOIN support** - System table joins with user data

## Running Tests

### Simple Test Execution

Run all tests with a single command:
```bash
./tests-integration/test.sh
```

This script will:
1. Build the datafusion-postgres project
2. Set up Python virtual environment and dependencies
3. Run CSV data loading tests
4. Run Parquet data loading tests
5. Verify PostgreSQL compatibility features

### Manual Test Execution

If you prefer to run tests manually:

1. **Build the project:**
   ```bash
   cargo build
   ```

2. **Set up Python environment:**
   ```bash
   cd tests-integration
   python3 -m venv test_env
   source test_env/bin/activate
   pip install psycopg
   ```

3. **Run individual tests:**
   ```bash
   # Test CSV data with PostgreSQL compatibility
   ../target/debug/datafusion-postgres-cli -p 5433 --csv delhi:delhiclimate.csv &
   python3 test_csv.py
   
   # Test Parquet data
   ../target/debug/datafusion-postgres-cli -p 5433 --parquet all_types:all_types.parquet &
   python3 test_parquet.py
   ```

## Test Results

When running `./test.sh`, you should see output like:

```
ğŸš€ Running DataFusion PostgreSQL Integration Tests
==================================================
Building datafusion-postgres...
Setting up Python dependencies...

ğŸ“Š Test 1: Enhanced CSV Data Loading & PostgreSQL Compatibility
----------------------------------------------------------------
ğŸ” Testing CSV data loading and PostgreSQL compatibility...

ğŸ“Š Basic Data Access Tests:
  âœ“ Delhi dataset count: 1462 rows
  âœ“ Limited query: 10 rows  
  âœ“ Parameterized query: 527 rows where meantemp > 30

ğŸ—‚ï¸ Enhanced pg_catalog Tests:
  âœ“ pg_catalog.pg_type: 16 data types
  âœ“ Core PostgreSQL types present: bool, int4, json, text, uuid
  âœ“ Table type detection: delhi = 'r' (regular table)
  âœ“ pg_attribute: 162 columns tracked

ğŸ”§ PostgreSQL Functions Tests:
  âœ“ version(): DataFusion PostgreSQL 48.0.0 on x86_64-pc-linux...
  âœ“ current_schema(): public
  âœ“ current_schemas(): ['public']
  âœ“ has_table_privilege(): True

âœ… Enhanced CSV test passed

ğŸ” Test 2: Transaction Support
------------------------------
ğŸ” Testing PostgreSQL Transaction Support

ğŸ“ Test 1: Basic Transaction Lifecycle
  âœ“ BEGIN executed
  âœ“ Query in transaction: 1462 rows
  âœ“ COMMIT executed
  âœ“ Query after commit works

ğŸ“ Test 2: Transaction Variants
  âœ“ BEGIN -> COMMIT
  âœ“ BEGIN TRANSACTION -> COMMIT TRANSACTION
  âœ“ ROLLBACK
  âœ“ ABORT

ğŸ“ Test 3: Failed Transaction Handling  
  âœ“ Transaction started
  âœ“ Invalid query failed as expected
  âœ“ Subsequent query blocked (error code 25P01)
  âœ“ ROLLBACK from failed transaction successful
  âœ“ Query execution restored after rollback

âœ… Transaction test passed

ğŸ“¦ Test 3: Enhanced Parquet Data Loading & Advanced Data Types
--------------------------------------------------------------
ğŸ” Testing Parquet data loading and advanced data types...

ğŸ“¦ Basic Parquet Data Tests:
  âœ“ all_types dataset count: 3 rows
  âœ“ Basic data retrieval: 1 rows
  âœ“ Full data access: 3 rows

ğŸ—ï¸ Data Type Compatibility Tests:
  âœ“ pg_catalog.pg_type: 16 data types
  âœ“ Enhanced types available: json, jsonb, uuid, interval, bytea
  âœ“ Column data types detected: 14 types

âœ… Enhanced Parquet test passed

ğŸ‰ All enhanced integration tests passed!
==========================================

ğŸ“ˆ Test Summary:
  âœ… Enhanced CSV data loading with PostgreSQL compatibility
  âœ… Complete transaction support (BEGIN/COMMIT/ROLLBACK)  
  âœ… Enhanced Parquet data loading with advanced data types
  âœ… Array types and complex data type support
  âœ… Improved pg_catalog system tables
  âœ… PostgreSQL function compatibility

ğŸš€ Ready for production PostgreSQL workloads!
```

## Tool Compatibility

These tests verify that datafusion-postgres works with PostgreSQL tools that expect:

- Standard system catalog tables (pg_catalog.*)
- PostgreSQL built-in functions
- information_schema views
- Proper error codes and messages
- Compatible data type handling

This allows tools like pgAdmin, DBeaver, and other PostgreSQL clients to work with datafusion-postgres.

## Test Data

- **`delhiclimate.csv`** - Sample CSV data with weather information (1462 rows)
- **`all_types.parquet`** - Parquet file with various Arrow data types for testing

## Notes

- Tests connect to port 5433 to avoid conflicts with existing PostgreSQL installations
- All tests use `autocommit = True` since datafusion-postgres doesn't support transactions yet
- Error handling tests verify proper PostgreSQL error codes are returned
- Type handling is flexible to accommodate differences between Arrow and PostgreSQL types
